标准的读取网页(注意这里面更改了格式 否则会乱码)

```python
url = 'https://www.ruanyifeng.com/blog/weekly/'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'}
response = requests.get(url=url, headers=headers)
if response.status_code == 200:
    response.encoding = "gb18030"
    # print(response.text)
html = response.content
```

通过选择器获取想要的信息

```python
soup = BeautifulSoup(html, 'lxml')
# print(soup.prettify())
module_list_item = soup.find_all(class_="module-list-item")
url_name = []
# 格式 [[网址，标题]]
for list_item in module_list_item:
    a = list_item.find_all(name='a')
    url_name.append([a[0].attrs['href'], a[0].string])
print(url_name)
```

保存为json文件 注意到也转换了格式，利用dump方法保存

```python
# 保存为json
filename = "url_name.json"
with open(filename, 'w', encoding='utf-8') as file_obj:
    json.dump(url_name, file_obj, ensure_ascii=False)
```

下面用load方法来读取

```python
import json

filename = 'url_name.json'
with open(filename, encoding="utf-8") as file_obj:
    url_name = json.load(file_obj)

for url in url_name:
    print(url)

```

> 1、python3里面默认编码是unicode
>
> 2、做dump与dumps操作时，会默认将中文转换为unicode，但在做逆向操作load和loads时会转换为中文，但是中间态(例如存储的json文件)的中文编码方式仍然是unicode
>
> 解决办法：
> 在dump里面添加`ensure_ascii=False`

